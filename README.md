# LocalLlama3
repo to run llama3 model in local mac M1 chip

Steps to install Ollama

1. download ollama in your local
reference:
https://www.linkedin.com/pulse/how-run-llama3-your-mac-silicon-chris-latimer-tvhuc/
https://www.llama.com/docs/llama-everywhere/running-meta-llama-on-mac/

2. install ollama and its cli
3. run ollama run llama3.1
    to specify model size

    ollama run llama3:8b
    ollama run llama3:70b
4. to close session close ollama instance

to serve the ollama to run without running desktop application
ollama serve


ollama run llama3.1
